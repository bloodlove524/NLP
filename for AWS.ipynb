{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanglongyi/anaconda2/envs/tensorflow36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "import math\n",
    "\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "CNNdev_sample_percentage = .1\n",
    "\n",
    "# Model Hyperparameters\n",
    "CNNenable_word_embeddings = True\n",
    "CNNembedding_dim = 128\n",
    "CNNfilter_sizes = '3,4,5'\n",
    "CNNnum_filters = 128\n",
    "CNNdropout_keep_prob = 0.5\n",
    "CNNl2_reg_lambda = 0.0\n",
    "\n",
    "# Training parameters\n",
    "CNNbatch_size = 64\n",
    "CNNnum_epochs = 200\n",
    "CNNevaluate_every = 100\n",
    "CNNcheckpoint_every = 100\n",
    "CNNnum_checkpoints = 5\n",
    "# Misc Parameters\n",
    "CNNallow_soft_placement = True\n",
    "CNNlog_device_placement = False\n",
    "CNNdecay_coefficient = 2.5\n",
    "\n",
    "#Pretrained Embeddings\n",
    "CNNword_embeddings = 'glove'\n",
    "CNNword_embeddings_path = 'data/golve.6B/glove.6B.100d.txt'\n",
    "CNNword_embeddings_dimension = 100\n",
    "CNNword_embeddings_length = 400000\n",
    "\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "\n",
    "def load_data_labels(datasets):\n",
    "    \"\"\"\n",
    "    Load data and labels\n",
    "    :param datasets:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Split by words\n",
    "    x_text = datasets['data']\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    # Generate labels\n",
    "    labels = []\n",
    "    for i in range(len(x_text)):\n",
    "        label = [0 for j in datasets['target_names']]\n",
    "        label[datasets['target'][i]] = 1\n",
    "        labels.append(label)\n",
    "    y = np.array(labels)\n",
    "    return [x_text, y]\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def load_embedding_vectors_glove(vocabulary, filename, vector_size):\n",
    "    # load embedding_vectors from the glove\n",
    "    # initial matrix with random uniform\n",
    "    embedding_vectors = np.random.uniform(-0.25, 0.25, (len(vocabulary), vector_size))\n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        idx = vocabulary.get(word)\n",
    "        if idx != 0:\n",
    "            embedding_vectors[idx] = vector\n",
    "    f.close()\n",
    "    return embedding_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#接入现成的词向量glove6B和30B\n",
    "import urllib\n",
    "from tqdm import tqdm\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "t0 = time()\n",
    "glove6B_zip_path = 'data/glove.6B.zip'\n",
    "glove6B_folder_path = 'data/golve.6B'\n",
    "        \n",
    "if not os.path.isfile(glove6B_zip_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='glove6B Dataset') as pbar:\n",
    "        urllib.request.urlretrieve(\n",
    "            'http://nlp.stanford.edu/data/glove.6B.zip',\n",
    "            glove6B_zip_path,\n",
    "            pbar.hook)\n",
    "    \n",
    "if not os.path.isdir(glove6B_folder_path):\n",
    "    with zipfile.ZipFile(glove6B_zip_path) as zipex:\n",
    "        zipex.extractall(glove6B_folder_path)\n",
    "        zipex.close()\n",
    "\n",
    "duration = time() - t0              \n",
    "print('All files ready!')\n",
    "print(\"Total glove-vectors download&extract time: %0.3f seconds\" % duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Vocabulary Size: 131739\n",
      "Train/Dev split: 10183/1131\n",
      "WARNING:tensorflow:From <ipython-input-2-3ec4372ecff2>:75: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/zhanglongyi/Documents/Personal/Udacity/news20/runs/1528951562\n",
      "\n",
      "Load glove file data/glove.6B.100d.txt\n",
      "glove file has been loaded\n",
      "\n",
      "2018-06-14T12:47:13.642374: step 1, loss 10.0675, acc 0.078125, learning_rate 0.005\n",
      "2018-06-14T12:48:08.219364: step 2, loss 9.22472, acc 0.0625, learning_rate 0.0049877\n",
      "2018-06-14T12:48:59.205294: step 3, loss 7.74702, acc 0.125, learning_rate 0.00497542\n",
      "2018-06-14T12:49:48.693435: step 4, loss 9.00634, acc 0.015625, learning_rate 0.00496318\n",
      "2018-06-14T12:50:39.986955: step 5, loss 7.11504, acc 0.046875, learning_rate 0.00495097\n",
      "2018-06-14T12:51:34.094935: step 6, loss 8.49686, acc 0.078125, learning_rate 0.00493879\n",
      "2018-06-14T12:52:29.968436: step 7, loss 7.76821, acc 0.015625, learning_rate 0.00492664\n",
      "2018-06-14T12:53:20.058449: step 8, loss 7.22408, acc 0.046875, learning_rate 0.00491452\n",
      "2018-06-14T12:54:09.449043: step 9, loss 6.43204, acc 0.046875, learning_rate 0.00490244\n",
      "2018-06-14T12:55:00.199373: step 10, loss 5.57925, acc 0.0625, learning_rate 0.00489038\n",
      "2018-06-14T12:55:51.436269: step 11, loss 5.57779, acc 0.09375, learning_rate 0.00487835\n",
      "2018-06-14T12:56:41.783985: step 12, loss 5.50841, acc 0.078125, learning_rate 0.00486635\n",
      "2018-06-14T12:57:34.865582: step 13, loss 5.80403, acc 0.03125, learning_rate 0.00485438\n",
      "2018-06-14T12:58:24.262491: step 14, loss 5.50192, acc 0.03125, learning_rate 0.00484245\n",
      "2018-06-14T12:59:11.339329: step 15, loss 5.17266, acc 0.09375, learning_rate 0.00483054\n",
      "2018-06-14T13:00:01.339324: step 16, loss 5.15177, acc 0.046875, learning_rate 0.00481866\n",
      "2018-06-14T13:00:51.326820: step 17, loss 5.24943, acc 0.078125, learning_rate 0.00480681\n",
      "2018-06-14T13:01:42.025861: step 18, loss 4.54524, acc 0.09375, learning_rate 0.004795\n",
      "2018-06-14T13:02:34.637139: step 19, loss 4.72917, acc 0.046875, learning_rate 0.00478321\n",
      "2018-06-14T13:31:27.724384: step 20, loss 4.35125, acc 0.125, learning_rate 0.00477145\n",
      "2018-06-14T14:03:18.162276: step 21, loss 4.76946, acc 0.09375, learning_rate 0.00475972\n",
      "2018-06-14T14:04:20.509483: step 22, loss 3.94638, acc 0.109375, learning_rate 0.00474802\n",
      "2018-06-14T14:05:18.674590: step 23, loss 4.04262, acc 0.15625, learning_rate 0.00473635\n",
      "2018-06-14T14:06:22.268960: step 24, loss 4.14, acc 0.09375, learning_rate 0.00472471\n",
      "2018-06-14T14:07:20.349432: step 25, loss 3.69497, acc 0.171875, learning_rate 0.0047131\n",
      "2018-06-14T14:08:20.828771: step 26, loss 3.68574, acc 0.140625, learning_rate 0.00470151\n",
      "2018-06-14T14:09:25.343049: step 27, loss 3.17584, acc 0.1875, learning_rate 0.00468996\n",
      "2018-06-14T14:10:20.180804: step 28, loss 3.57403, acc 0.15625, learning_rate 0.00467844\n",
      "2018-06-14T14:11:13.144766: step 29, loss 3.08662, acc 0.21875, learning_rate 0.00466694\n",
      "2018-06-14T14:12:05.655032: step 30, loss 3.20726, acc 0.265625, learning_rate 0.00465547\n",
      "2018-06-14T14:12:56.616063: step 31, loss 3.40161, acc 0.15625, learning_rate 0.00464404\n",
      "2018-06-14T14:13:54.969097: step 32, loss 3.30344, acc 0.1875, learning_rate 0.00463263\n",
      "2018-06-14T14:14:54.841731: step 33, loss 3.20677, acc 0.171875, learning_rate 0.00462125\n",
      "2018-06-14T14:15:44.767751: step 34, loss 3.26185, acc 0.140625, learning_rate 0.00460989\n",
      "2018-06-14T14:16:38.923453: step 35, loss 2.99448, acc 0.171875, learning_rate 0.00459857\n",
      "2018-06-14T14:17:32.203648: step 36, loss 3.21174, acc 0.09375, learning_rate 0.00458727\n",
      "2018-06-14T14:18:28.347532: step 37, loss 2.75377, acc 0.3125, learning_rate 0.00457601\n",
      "2018-06-14T14:19:24.357674: step 38, loss 3.10571, acc 0.1875, learning_rate 0.00456477\n",
      "2018-06-14T14:20:29.246667: step 39, loss 2.90514, acc 0.265625, learning_rate 0.00455356\n",
      "2018-06-14T14:21:28.932394: step 40, loss 2.56295, acc 0.3125, learning_rate 0.00454238\n"
     ]
    }
   ],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "\n",
    "\n",
    "\n",
    "if CNNenable_word_embeddings:\n",
    "    embedding_name = CNNword_embeddings\n",
    "    embedding_dimension = CNNword_embeddings_dimension\n",
    "else:\n",
    "    embedding_dimension = CNNembedding_dim\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "categories = ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware',\n",
    "                  'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles',\n",
    "                  'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med',\n",
    "                  'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast',\n",
    "                  'talk.politics.misc', 'talk.religion.misc']\n",
    "datasets = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42, remove=())\n",
    "\n",
    "x_text, y = load_data_labels(datasets)\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(CNNdev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=CNNallow_soft_placement,\n",
    "                                  log_device_placement=CNNlog_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=embedding_dimension,\n",
    "            filter_sizes=list(map(int, CNNfilter_sizes.split(\",\"))),\n",
    "            num_filters=CNNnum_filters,\n",
    "            l2_reg_lambda=CNNl2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(cnn.learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=CNNnum_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if CNNenable_word_embeddings:\n",
    "            vocabulary = vocab_processor.vocabulary_\n",
    "            initW = None\n",
    "            # load embedding vectors from the glove\n",
    "            print(\"Load glove file {}\".format(CNNword_embeddings_path))\n",
    "            initW = load_embedding_vectors_glove(vocabulary,CNNword_embeddings_path, embedding_dimension)\n",
    "            print(\"glove file has been loaded\\n\")\n",
    "            sess.run(cnn.W.assign(initW))\n",
    "\n",
    "        def train_step(x_batch, y_batch, learning_rate):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: CNNdropout_keep_prob,\n",
    "              cnn.learning_rate: learning_rate\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, learning_rate {:g}\"\n",
    "                  .format(time_str, step, loss, accuracy, learning_rate))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), CNNbatch_size, CNNnum_epochs)\n",
    "        # It uses dynamic learning rate with a high value at the beginning to speed up the training\n",
    "        max_learning_rate = 0.005\n",
    "        min_learning_rate = 0.0001\n",
    "        decay_speed = CNNdecay_coefficient*len(y_train)/CNNbatch_size\n",
    "        # Training loop. For each batch...\n",
    "        counter = 0\n",
    "        for batch in batches:\n",
    "            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-counter/decay_speed)\n",
    "            counter += 1\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch, learning_rate)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % CNNevaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % CNNcheckpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.contrib import learn\n",
    "import csv\n",
    "from sklearn import metrics\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape((1, -1))\n",
    "    max_x = np.max(x, axis=1).reshape((-1, 1))\n",
    "    exp_x = np.exp(x - max_x)\n",
    "    return exp_x / np.sum(exp_x, axis=1).reshape((-1, 1))\n",
    "\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data Parameters\n",
    "\n",
    "# Eval Parameters\n",
    "CNNbatch_size = 64\n",
    "CNNcheckpoint_dir = \"runs/1528898196/checkpoints/model-300\"\n",
    "CNNeval_train = True\n",
    "\n",
    "# Misc Parameters\n",
    "CNNallow_soft_placement = True\n",
    "CNNlog_device_placement = False\n",
    "\n",
    "datasets = None\n",
    "\n",
    "# CHANGE THIS: Load data. Load your own data here\n",
    "if CNNeval_train:\n",
    "    datasets = fetch_20newsgroups(subset=\"test\",categories=catogories,shuffle=True, random_state=42, remove=())\n",
    "    x_raw, y_test = load_data_labels(datasets)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "else:\n",
    "    datasets = {\"target_names\": ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']}\n",
    "    x_raw = [\"The number of reported cases of gonorrhea in Colorado increased\",\n",
    "             \"I am in the market for a 24-bit graphics card for a PC\"]\n",
    "    y_test = [2, 1]\n",
    "\n",
    "# Map data into vocabulary\n",
    "vocab_path = os.path.join(CNNcheckpoint_dir, \"..\", \"vocab\")\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))\n",
    "\n",
    "print(\"\\nEvaluating...\\n\")\n",
    "\n",
    "# Evaluation\n",
    "# ==================================================\n",
    "checkpoint_file = tf.train.latest_checkpoint(CNNcheckpoint_dir)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=CNNallow_soft_placement,\n",
    "      log_device_placement=CNNlog_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        scores = graph.get_operation_by_name(\"output/scores\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = batch_iter(list(x_test), CNNbatch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "        all_probabilities = None\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions_scores = sess.run([predictions, scores], {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions_scores[0]])\n",
    "            probabilities = softmax(batch_predictions_scores[1])\n",
    "            if all_probabilities is not None:\n",
    "                all_probabilities = np.concatenate([all_probabilities, probabilities])\n",
    "            else:\n",
    "                all_probabilities = probabilities\n",
    "\n",
    "# Print accuracy if y_test is defined\n",
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))\n",
    "    print(metrics.classification_report(y_test, all_predictions, target_names=datasets['target_names']))\n",
    "    print(metrics.confusion_matrix(y_test, all_predictions))\n",
    "\n",
    "# Save the evaluation to a csv\n",
    "predictions_human_readable = np.column_stack((np.array(x_raw),\n",
    "                                              [int(prediction) for prediction in all_predictions],\n",
    "                                              [ \"{}\".format(probability) for probability in all_probabilities]))\n",
    "out_path = os.path.join(CNNcheckpoint_dir, \"..\", \"prediction.csv\")\n",
    "print(\"Saving evaluation to {0}\".format(out_path))\n",
    "with open(out_path, 'w') as f:\n",
    "    csv.writer(f).writerows(predictions_human_readable)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
